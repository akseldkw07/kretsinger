{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from /Users/Akseldkw/coding/kretsinger/.env\n",
      "[kret_matplotlib.mpl_nb_imports] Imported kret_matplotlib.mpl_nb_imports in 2.2127 seconds\n",
      "[kret_np_pd.np_pd_nb_imports] Imported kret_np_pd.np_pd_nb_imports in 1.1358 seconds\n",
      "[kret_sklearn.sklearn_nb_imports] Imported kret_sklearn.sklearn_nb_imports in 0.5304 seconds\n",
      "[kret_torch_utils.torch_nb_imports] Imported kret_torch_utils.torch_nb_imports in 2.9108 seconds\n",
      "[kret_lightning.lightning_nb_imports] Imported kret_lightning.lightning_nb_imports in 0.1362 seconds\n",
      "[kret_tqdm.tqdm_nb_imports] Imported kret_tqdm.tqdm_nb_imports in 0.0000 seconds\n",
      "[kret_type_hints.types_nb_imports] Imported kret_type_hints.types_nb_imports in 0.0024 seconds\n",
      "[kret_utils.utils_nb_imports] Imported kret_utils.utils_nb_imports in 0.0007 seconds\n"
     ]
    }
   ],
   "source": [
    "from kret_notebook import *  # NOTE import first\n",
    "from kret_matplotlib.mpl_nb_imports import *\n",
    "from kret_np_pd.np_pd_nb_imports import *\n",
    "from kret_sklearn.sklearn_nb_imports import *\n",
    "from kret_torch_utils.torch_nb_imports import *\n",
    "from kret_lightning.lightning_nb_imports import *\n",
    "from kret_tqdm.tqdm_nb_imports import *\n",
    "from kret_type_hints.types_nb_imports import *\n",
    "from kret_utils.utils_nb_imports import *\n",
    "\n",
    "# from kret_wandb.wandb_nb_imports import *  # NOTE this is slow to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_data import MNISTDataModule\n",
    "\n",
    "mnist_data_module = MNISTDataModule(DATA_DIR / \"MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kret_lightning.base_lightning_nn import HPasKwargs\n",
    "\n",
    "\n",
    "class Kret_AutoEncoder(BaseLightningNN):\n",
    "    _criterion: nn.Module = nn.MSELoss()\n",
    "\n",
    "    def __init__(self, embedding_dim: tuple[int, int], **kwargs: t.Unpack[HPasKwargs]):\n",
    "        super().__init__(**kwargs)\n",
    "        # define any number of nn.Modules (or use your current ones)\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(*embedding_dim))\n",
    "        self.decoder = nn.Sequential(nn.Linear(embedding_dim[1], embedding_dim[0]), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    # endregion\n",
    "    # region Training / Validation Steps\n",
    "    def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        loss = ...\n",
    "        return loss\n",
    "        \"\"\"\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        outputs = self(x)\n",
    "        loss = self.get_loss(outputs, x)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        val_loss = ...\n",
    "        self.log('val_loss', val_loss)\n",
    "        \"\"\"\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        outputs = self(x)\n",
    "        val_loss = self.get_loss(outputs, x)\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    # endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Kret_AutoEncoder((64, 3))\n",
    "auto_enc = base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'save_dir': PosixPath('/Users/Akseldkw/coding/data_kretsinger/lightning_logs'),\n",
       " 'name': 'Kret_AutoEncoder__lr=0.001--gamma=0.5--stepsize=12--embedding_dim=(64,3)',\n",
       " 'version': 'v_000'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.save_load_logging_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(**base.save_load_logging_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "static_args = TrainerStaticDefaults.TRAINER_QUICK_ITER\n",
    "dynamic_args = TrainerDynamicDefaults.trainer_dynamic_defaults(auto_enc, mnist_data_module)\n",
    "trainer_args = static_args | dynamic_args\n",
    "trainer = L.Trainer(**trainer_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ encoder │ Sequential │ 50.4 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ decoder │ Sequential │ 51.2 K │ train │     0 │\n",
       "└───┴─────────┴────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ encoder │ Sequential │ 50.4 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ decoder │ Sequential │ 51.2 K │ train │     0 │\n",
       "└───┴─────────┴────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 101 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 101 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 8                                                                                           \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 101 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 101 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 8                                                                                           \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c836408d1e049ffb856d264c874e42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=auto_enc, datamodule=mnist_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kret_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
