{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from /Users/Akseldkw/coding/kretsinger/.env\n",
      "[kret_type_hints.types_nb_imports] Imported kret_type_hints.types_nb_imports in 0.0056 seconds\n",
      "[kret_utils.utils_nb_imports] Imported kret_utils.utils_nb_imports in 0.6888 seconds\n"
     ]
    }
   ],
   "source": [
    "from kret_notebook import *  # NOTE import first\n",
    "\n",
    "# from kret_matplotlib.mpl_nb_imports import *\n",
    "# from kret_np_pd.np_pd_nb_imports import *\n",
    "# from kret_sklearn.sklearn_nb_imports import *\n",
    "# from kret_torch_utils.torch_nb_imports import *\n",
    "# from kret_lightning.lightning_nb_imports import *\n",
    "# from kret_tqdm.tqdm_nb_imports import *\n",
    "from kret_type_hints.types_nb_imports import *\n",
    "from kret_utils.utils_nb_imports import *\n",
    "\n",
    "# from kret_wandb.wandb_nb_imports import *  # NOTE this is slow to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger, WandbLogger, TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kret_sandbox.func_to_typed_dict import FuncToTypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testfunc(\n",
    "    a: int,\n",
    "    df: pd.DataFrame,  # TODO Had to fix\n",
    "    mod: nn.Module,\n",
    "    b: str | None = \"default\",\n",
    "    c: t.Literal[\"a\", \"b\", \"c\"] | bool = False,\n",
    "    mylist: list[t.Literal[0, 1]] = [0],\n",
    ") -> None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pd.DataFrame\n",
      "import t.Literal\n",
      "import torch.nn.modules.module.Module\n",
      "class Testfunc_TypedDict(t.TypedDict, total=False):\n",
      "    a: int\n",
      "    df: pd.DataFrame\n",
      "    mod: torch.nn.modules.module.Module\n",
      "    b: str | None\n",
      "    c: t.Literal['a' | 'b' | 'c'] | bool\n",
      "    mylist: list[t.Literal[0 | 1]]\n"
     ]
    }
   ],
   "source": [
    "UKS_TH_UTILS.func_to_typed_dict(testfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'torch.nn.modules.module.Module'>\n",
      "list[typing.Literal[0, 1]]\n",
      "str | None\n",
      "typing.Union[typing.Literal['a', 'b', 'c'], bool]\n",
      "class Testfunc_TypedDict(TypedDict):\n",
      "    a: int\n",
      "    df: DataFrame\n",
      "    mod: Module\n",
      "    b: UnionType[str, None]\n",
      "    c: Literal[a, b, c] | bool\n",
      "    mylist: list[Literal[0, 1]]\n"
     ]
    }
   ],
   "source": [
    "FuncToTypedDict.func_to_typed_dict(testfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "\n",
    "class Testfunc_TypedDict(t.TypedDict, total=False):\n",
    "    a: int\n",
    "    b: str | None\n",
    "    c: t.Literal[\"a\", \"b\", \"c\"] | bool  # TODO Had to fix\n",
    "    mylist: list[t.Literal[0, 1]]  # TODO Had to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import collections.abc.Iterable\n",
      "import datetime.timedelta\n",
      "import lightning.fabric.plugins.environments.cluster_environment.ClusterEnvironment\n",
      "import lightning.fabric.plugins.io.checkpoint_io.CheckpointIO\n",
      "import lightning.pytorch.accelerators.accelerator.Accelerator\n",
      "import lightning.pytorch.callbacks.callback.Callback\n",
      "import lightning.pytorch.loggers.logger.Logger\n",
      "import lightning.pytorch.plugins.layer_sync.LayerSync\n",
      "import lightning.pytorch.plugins.precision.precision.Precision\n",
      "import lightning.pytorch.profilers.profiler.Profiler\n",
      "import lightning.pytorch.strategies.strategy.Strategy\n",
      "import pathlib.Path\n",
      "import t.Any\n",
      "import t.Literal\n",
      "class __init___TypedDict(t.TypedDict, total=False):\n",
      "    self: t.Any\n",
      "    accelerator: str | lightning.pytorch.accelerators.accelerator.Accelerator\n",
      "    strategy: str | lightning.pytorch.strategies.strategy.Strategy\n",
      "    devices: list[int] | str | int\n",
      "    num_nodes: int\n",
      "    precision: t.Literal[64 | 32 | 16] | t.Literal['transformer-engine' | 'transformer-engine-float16' | '16-true' | '16-mixed' | 'bf16-true' | 'bf16-mixed' | '32-true' | '64-true'] | t.Literal['64' | '32' | '16' | 'bf16'] | None\n",
      "    logger: lightning.pytorch.loggers.logger.Logger | collections.abc.Iterable[lightning.pytorch.loggers.logger.Logger] | bool | None\n",
      "    callbacks: list[lightning.pytorch.callbacks.callback.Callback] | lightning.pytorch.callbacks.callback.Callback | None\n",
      "    fast_dev_run: int | bool\n",
      "    max_epochs: int | None\n",
      "    min_epochs: int | None\n",
      "    max_steps: int\n",
      "    min_steps: int | None\n",
      "    max_time: str | datetime.timedelta | dict[str | int] | None\n",
      "    limit_train_batches: int | float | None\n",
      "    limit_val_batches: int | float | None\n",
      "    limit_test_batches: int | float | None\n",
      "    limit_predict_batches: int | float | None\n",
      "    overfit_batches: int | float\n",
      "    val_check_interval: int | float | str | datetime.timedelta | dict[str | int] | None\n",
      "    check_val_every_n_epoch: int | None\n",
      "    num_sanity_val_steps: int | None\n",
      "    log_every_n_steps: int | None\n",
      "    enable_checkpointing: bool | None\n",
      "    enable_progress_bar: bool | None\n",
      "    enable_model_summary: bool | None\n",
      "    accumulate_grad_batches: int\n",
      "    gradient_clip_val: int | float | None\n",
      "    gradient_clip_algorithm: str | None\n",
      "    deterministic: bool | t.Literal['warn'] | None\n",
      "    benchmark: bool | None\n",
      "    inference_mode: bool\n",
      "    use_distributed_sampler: bool\n",
      "    profiler: lightning.pytorch.profilers.profiler.Profiler | str | None\n",
      "    detect_anomaly: bool\n",
      "    barebones: bool\n",
      "    plugins: lightning.pytorch.plugins.precision.precision.Precision | lightning.fabric.plugins.environments.cluster_environment.ClusterEnvironment | lightning.fabric.plugins.io.checkpoint_io.CheckpointIO | lightning.pytorch.plugins.layer_sync.LayerSync | list[lightning.pytorch.plugins.precision.precision.Precision | lightning.pytorch.plugins.layer_sync.LayerSync] | None\n",
      "    sync_batchnorm: bool\n",
      "    reload_dataloaders_every_n_epochs: int\n",
      "    default_root_dir: str | pathlib.Path | None\n",
      "    enable_autolog_hparams: bool\n",
      "    model_registry: str | None\n"
     ]
    }
   ],
   "source": [
    "UKS_TH_UTILS.func_to_typed_dict(L.Trainer.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from lightningdatamodule import LightningDataModule\n",
      "from lightningmodule import LightningModule\n",
      "from nonetype import NoneType\n",
      "from path import Path\n",
      "class Trainer_Fit_TypedDict(TypedDict):\n",
      "    model: pl.LightningModule\n",
      "    train_dataloaders: Any | LightningDataModule | None\n",
      "    val_dataloaders: Any | None\n",
      "    datamodule: LightningDataModule | None\n",
      "    ckpt_path: str | Path | None\n",
      "    weights_only: bool | None\n"
     ]
    }
   ],
   "source": [
    "FuncToTypedDict.func_to_typed_dict(L.Trainer.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'inspect._empty'>\n",
      "pl.LightningModule\n",
      "typing.Optional[bool]\n",
      "typing.Optional[lightning.pytorch.core.datamodule.LightningDataModule]\n",
      "typing.Optional[typing.Any]\n",
      "typing.Union[str, pathlib.Path, NoneType]\n",
      "typing.Union[typing.Any, lightning.pytorch.core.datamodule.LightningDataModule, NoneType]\n"
     ]
    }
   ],
   "source": [
    "FuncToTypedDict.print_imports(L.Trainer.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kret_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
