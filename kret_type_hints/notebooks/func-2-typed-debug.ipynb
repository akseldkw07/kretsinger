{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kret_notebook import *  # NOTE import first\n",
    "\n",
    "# from kret_matplotlib.mpl_nb_imports import *\n",
    "# from kret_np_pd.np_pd_nb_imports import *\n",
    "# from kret_sklearn.sklearn_nb_imports import *\n",
    "# from kret_torch_utils.torch_nb_imports import *\n",
    "# from kret_lightning.lightning_nb_imports import *\n",
    "# from kret_tqdm.tqdm_nb_imports import *\n",
    "from kret_type_hints.types_nb_imports import *\n",
    "from kret_utils.utils_nb_imports import *\n",
    "\n",
    "# from kret_wandb.wandb_nb_imports import *  # NOTE this is slow to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger, WandbLogger, TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kret_sandbox.func_to_typed_dict import FuncToTypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testfunc(\n",
    "    a: int,\n",
    "    df: pd.DataFrame,  # TODO Had to fix\n",
    "    mod: nn.Module,\n",
    "    b: str | None = \"default\",\n",
    "    c: t.Literal[\"a\", \"b\", \"c\"] | bool = False,\n",
    "    mylist: list[t.Literal[0, 1]] = [0],\n",
    ") -> None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('torch.nn', 'Module')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FuncToTypedDict._resolve_import_location(torch.nn.modules.module.Module, \"Module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pandas import DataFrame\n",
      "from torch.nn import Module\n",
      "from typing import Literal\n",
      "from typing import TypedDict\n",
      "\n",
      "class Testfunc_TypedDict(TypedDict):\n",
      "    a: int\n",
      "    df: DataFrame\n",
      "    mod: Module\n",
      "    b: str | None\n",
      "    c: Literal['a', 'b', 'c'] | bool\n",
      "    mylist: list[Literal[0, 1]]\n"
     ]
    }
   ],
   "source": [
    "FuncToTypedDict.func_to_typed_dict(testfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from torch.nn import Module\n",
    "from typing import Literal\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class Testfunc_TypedDict(TypedDict):\n",
    "    a: int\n",
    "    df: DataFrame\n",
    "    mod: Module\n",
    "    b: str | None\n",
    "    c: Literal[\"a\", \"b\", \"c\"] | bool\n",
    "    mylist: list[Literal[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from collections.abc import Iterable\n",
      "from datetime import timedelta\n",
      "from lightning import Callback\n",
      "from lightning.fabric.plugins import CheckpointIO\n",
      "from lightning.fabric.plugins import ClusterEnvironment\n",
      "from lightning.pytorch.accelerators import Accelerator\n",
      "from lightning.pytorch.loggers import Logger\n",
      "from lightning.pytorch.plugins import LayerSync\n",
      "from lightning.pytorch.plugins import Precision\n",
      "from lightning.pytorch.profilers import Profiler\n",
      "from lightning.pytorch.strategies import Strategy\n",
      "from pathlib import Path\n",
      "from typing import Literal\n",
      "from typing import TypedDict\n",
      "\n",
      "class Trainer___init___TypedDict(TypedDict):\n",
      "    accelerator: str | Accelerator\n",
      "    strategy: str | Strategy\n",
      "    devices: list[int] | str | int\n",
      "    num_nodes: int\n",
      "    precision: Literal[64, 32, 16] | Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'] | Literal['64', '32', '16', 'bf16'] | None\n",
      "    logger: Logger | Iterable[Logger] | bool | None\n",
      "    callbacks: list[Callback] | Callback | None\n",
      "    fast_dev_run: int | bool\n",
      "    max_epochs: int | None\n",
      "    min_epochs: int | None\n",
      "    max_steps: int\n",
      "    min_steps: int | None\n",
      "    max_time: str | timedelta | dict[str, int] | None\n",
      "    limit_train_batches: int | float | None\n",
      "    limit_val_batches: int | float | None\n",
      "    limit_test_batches: int | float | None\n",
      "    limit_predict_batches: int | float | None\n",
      "    overfit_batches: int | float\n",
      "    val_check_interval: int | float | str | timedelta | dict[str, int] | None\n",
      "    check_val_every_n_epoch: int | None\n",
      "    num_sanity_val_steps: int | None\n",
      "    log_every_n_steps: int | None\n",
      "    enable_checkpointing: bool | None\n",
      "    enable_progress_bar: bool | None\n",
      "    enable_model_summary: bool | None\n",
      "    accumulate_grad_batches: int\n",
      "    gradient_clip_val: int | float | None\n",
      "    gradient_clip_algorithm: str | None\n",
      "    deterministic: bool | Literal['warn'] | None\n",
      "    benchmark: bool | None\n",
      "    inference_mode: bool\n",
      "    use_distributed_sampler: bool\n",
      "    profiler: Profiler | str | None\n",
      "    detect_anomaly: bool\n",
      "    barebones: bool\n",
      "    plugins: Precision | ClusterEnvironment | CheckpointIO | LayerSync | list[Precision | ClusterEnvironment | CheckpointIO | LayerSync] | None\n",
      "    sync_batchnorm: bool\n",
      "    reload_dataloaders_every_n_epochs: int\n",
      "    default_root_dir: str | Path | None\n",
      "    enable_autolog_hparams: bool\n",
      "    model_registry: str | None\n"
     ]
    }
   ],
   "source": [
    "FuncToTypedDict.func_to_typed_dict(L.Trainer.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from datetime import timedelta\n",
    "from lightning import Callback\n",
    "from lightning.fabric.plugins import CheckpointIO\n",
    "from lightning.fabric.plugins import ClusterEnvironment\n",
    "from lightning.pytorch.accelerators import Accelerator\n",
    "from lightning.pytorch.loggers import Logger\n",
    "from lightning.pytorch.plugins import LayerSync\n",
    "from lightning.pytorch.plugins import Precision\n",
    "from lightning.pytorch.profilers import Profiler\n",
    "from lightning.pytorch.strategies import Strategy\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class Trainer___init___TypedDict(TypedDict):\n",
    "    accelerator: str | Accelerator\n",
    "    strategy: str | Strategy\n",
    "    devices: list[int] | str | int\n",
    "    num_nodes: int\n",
    "    precision: (\n",
    "        Literal[64, 32, 16]\n",
    "        | Literal[\n",
    "            \"transformer-engine\",\n",
    "            \"transformer-engine-float16\",\n",
    "            \"16-true\",\n",
    "            \"16-mixed\",\n",
    "            \"bf16-true\",\n",
    "            \"bf16-mixed\",\n",
    "            \"32-true\",\n",
    "            \"64-true\",\n",
    "        ]\n",
    "        | Literal[\"64\", \"32\", \"16\", \"bf16\"]\n",
    "        | None\n",
    "    )\n",
    "    logger: Logger | Iterable[Logger] | bool | None\n",
    "    callbacks: list[Callback] | Callback | None\n",
    "    fast_dev_run: int | bool\n",
    "    max_epochs: int | None\n",
    "    min_epochs: int | None\n",
    "    max_steps: int\n",
    "    min_steps: int | None\n",
    "    max_time: str | timedelta | dict[str, int] | None\n",
    "    limit_train_batches: int | float | None\n",
    "    limit_val_batches: int | float | None\n",
    "    limit_test_batches: int | float | None\n",
    "    limit_predict_batches: int | float | None\n",
    "    overfit_batches: int | float\n",
    "    val_check_interval: int | float | str | timedelta | dict[str, int] | None\n",
    "    check_val_every_n_epoch: int | None\n",
    "    num_sanity_val_steps: int | None\n",
    "    log_every_n_steps: int | None\n",
    "    enable_checkpointing: bool | None\n",
    "    enable_progress_bar: bool | None\n",
    "    enable_model_summary: bool | None\n",
    "    accumulate_grad_batches: int\n",
    "    gradient_clip_val: int | float | None\n",
    "    gradient_clip_algorithm: str | None\n",
    "    deterministic: bool | Literal[\"warn\"] | None\n",
    "    benchmark: bool | None\n",
    "    inference_mode: bool\n",
    "    use_distributed_sampler: bool\n",
    "    profiler: Profiler | str | None\n",
    "    detect_anomaly: bool\n",
    "    barebones: bool\n",
    "    plugins: (\n",
    "        Precision\n",
    "        | ClusterEnvironment\n",
    "        | CheckpointIO\n",
    "        | LayerSync\n",
    "        | list[Precision | ClusterEnvironment | CheckpointIO | LayerSync]\n",
    "        | None\n",
    "    )\n",
    "    sync_batchnorm: bool\n",
    "    reload_dataloaders_every_n_epochs: int\n",
    "    default_root_dir: str | Path | None\n",
    "    enable_autolog_hparams: bool\n",
    "    model_registry: str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from lightning import LightningDataModule\n",
      "from pathlib import Path\n",
      "from typing import Any\n",
      "from typing import TypedDict\n",
      "\n",
      "class Trainer_Fit_TypedDict(TypedDict):\n",
      "    model: 'pl.LightningModule'\n",
      "    train_dataloaders: Any | LightningDataModule | None\n",
      "    val_dataloaders: Any | None\n",
      "    datamodule: LightningDataModule | None\n",
      "    ckpt_path: str | Path | None\n",
      "    weights_only: bool | None\n"
     ]
    }
   ],
   "source": [
    "FuncToTypedDict.func_to_typed_dict(L.Trainer.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningDataModule\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class Trainer_Fit_TypedDict(TypedDict):\n",
    "    model: \"pl.LightningModule\"\n",
    "    train_dataloaders: Any | LightningDataModule | None\n",
    "    val_dataloaders: Any | None\n",
    "    datamodule: LightningDataModule | None\n",
    "    ckpt_path: str | Path | None\n",
    "    weights_only: bool | None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kret_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
