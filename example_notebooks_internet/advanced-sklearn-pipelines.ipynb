{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Advanced Pipelines\n",
    "\n",
    "This goal of this notebook is to provide examples of advanced pipeline functionality that I find useful on most all machine learning projects. You can use this notebook as a utility script and import the pipeline components for use in your modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T05:17:13.420154Z",
     "iopub.status.busy": "2023-10-18T05:17:13.419792Z",
     "iopub.status.idle": "2023-10-18T05:17:23.430696Z",
     "shell.execute_reply": "2023-10-18T05:17:23.429804Z",
     "shell.execute_reply.started": "2023-10-18T05:17:13.420128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlforecast\n",
      "  Downloading mlforecast-1.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hierarchicalforecast\n",
      "  Downloading hierarchicalforecast-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: cloudpickle in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from mlforecast) (3.1.2)\n",
      "Collecting coreforecast>=0.0.15 (from mlforecast)\n",
      "  Downloading coreforecast-0.0.16-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: fsspec in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from mlforecast) (2025.10.0)\n",
      "Requirement already satisfied: optuna in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from mlforecast) (4.6.0)\n",
      "Requirement already satisfied: pandas in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from mlforecast) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from mlforecast) (1.7.2)\n",
      "Collecting utilsforecast>=0.2.9 (from mlforecast)\n",
      "  Downloading utilsforecast-0.2.15-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from hierarchicalforecast) (2.3.5)\n",
      "Requirement already satisfied: numba in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from hierarchicalforecast) (0.62.1)\n",
      "Requirement already satisfied: matplotlib in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from hierarchicalforecast) (3.10.8)\n",
      "Requirement already satisfied: narwhals>=2.0 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from hierarchicalforecast) (2.12.0)\n",
      "Collecting qpsolvers[clarabel] (from hierarchicalforecast)\n",
      "  Downloading qpsolvers-4.8.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from pandas->mlforecast) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from pandas->mlforecast) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from pandas->mlforecast) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->mlforecast) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from scikit-learn->mlforecast) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from scikit-learn->mlforecast) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from scikit-learn->mlforecast) (3.6.0)\n",
      "Requirement already satisfied: packaging in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from utilsforecast>=0.2.9->mlforecast) (25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from matplotlib->hierarchicalforecast) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from matplotlib->hierarchicalforecast) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from matplotlib->hierarchicalforecast) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from matplotlib->hierarchicalforecast) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from matplotlib->hierarchicalforecast) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from matplotlib->hierarchicalforecast) (3.2.5)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from numba->hierarchicalforecast) (0.45.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from optuna->mlforecast) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from optuna->mlforecast) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from optuna->mlforecast) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from optuna->mlforecast) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from optuna->mlforecast) (6.0.3)\n",
      "Requirement already satisfied: Mako in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->mlforecast) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->mlforecast) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna->mlforecast) (3.0.3)\n",
      "Requirement already satisfied: clarabel>=0.4.1 in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from qpsolvers[clarabel]->hierarchicalforecast) (0.11.1)\n",
      "Requirement already satisfied: cffi in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from clarabel>=0.4.1->qpsolvers[clarabel]->hierarchicalforecast) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/Akseldkw/micromamba/envs/kret_312/lib/python3.12/site-packages (from cffi->clarabel>=0.4.1->qpsolvers[clarabel]->hierarchicalforecast) (2.22)\n",
      "Downloading mlforecast-1.0.2-py3-none-any.whl (72 kB)\n",
      "Downloading hierarchicalforecast-1.3.1-py3-none-any.whl (54 kB)\n",
      "Downloading coreforecast-0.0.16-cp312-cp312-macosx_11_0_arm64.whl (225 kB)\n",
      "Downloading utilsforecast-0.2.15-py3-none-any.whl (40 kB)\n",
      "Downloading qpsolvers-4.8.2-py3-none-any.whl (92 kB)\n",
      "Installing collected packages: coreforecast, qpsolvers, utilsforecast, mlforecast, hierarchicalforecast\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [hierarchicalforecast]\n",
      "\u001b[1A\u001b[2KSuccessfully installed coreforecast-0.0.16 hierarchicalforecast-1.3.1 mlforecast-1.0.2 qpsolvers-4.8.2 utilsforecast-0.2.15\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install mlforecast hierarchicalforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T05:21:36.114782Z",
     "iopub.status.busy": "2023-10-18T05:21:36.113801Z",
     "iopub.status.idle": "2023-10-18T05:21:36.120411Z",
     "shell.execute_reply": "2023-10-18T05:21:36.119399Z",
     "shell.execute_reply.started": "2023-10-18T05:21:36.114746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from mlforecast.target_transforms import BaseTargetTransform\n",
    "\n",
    "from pandas.tseries.offsets import Day\n",
    "\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for loading CSV files as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T05:47:54.743841Z",
     "iopub.status.busy": "2023-10-18T05:47:54.743438Z",
     "iopub.status.idle": "2023-10-18T05:47:54.771412Z",
     "shell.execute_reply": "2023-10-18T05:47:54.770465Z",
     "shell.execute_reply.started": "2023-10-18T05:47:54.743808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_start = pd.Timestamp(2013, 1, 1)\n",
    "_end = pd.Timestamp(2017, 8, 31)\n",
    "_CUTOFF = pd.Timestamp(2017, 8, 15)\n",
    "\n",
    "_TRAIN_DATETIME_INDEX = pd.date_range(_start, _CUTOFF, freq=\"D\", name=\"date\")\n",
    "\n",
    "_TEST_DATETIME_INDEX = pd.date_range(_CUTOFF + (1 * Day()), _end, freq=\"D\", name=\"date\")\n",
    "\n",
    "_COMBINED_DATETIME_INDEX = pd.date_range(_start, _end, freq=\"D\", name=\"date\")\n",
    "\n",
    "\n",
    "def load_train_df(\n",
    "    filepath=\"/kaggle/input/store-sales-time-series-forecasting/train.csv\", onpromotion=False\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    if onpromotion:\n",
    "        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\", \"sales\", \"onpromotion\"]\n",
    "    else:\n",
    "        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\", \"sales\"]\n",
    "\n",
    "    df = (\n",
    "        pd.read_csv(\n",
    "            filepath,\n",
    "            usecols=_usecols,\n",
    "            parse_dates=[\"date\"],\n",
    "            dtype={\n",
    "                \"store_nbr\": str,\n",
    "                \"sales\": np.float32,\n",
    "            },\n",
    "        )\n",
    "        .pipe(lambda df: df.assign(family=df.loc[:, \"family\"].str.replace(\"/\", \" OR \")))\n",
    "        .set_index([\"store_nbr\", \"family\", \"date\"])\n",
    "        .reindex(_TRAIN_DATETIME_INDEX, level=\"date\")\n",
    "        .sort_index(axis=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_test_df(\n",
    "    filepath=\"/kaggle/input/store-sales-time-series-forecasting/test.csv\", onpromotion=False\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    if onpromotion:\n",
    "        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\", \"onpromotion\"]\n",
    "    else:\n",
    "        _usecols = [\"id\", \"store_nbr\", \"family\", \"date\"]\n",
    "\n",
    "    df = (\n",
    "        pd.read_csv(\n",
    "            filepath,\n",
    "            usecols=_usecols,\n",
    "            parse_dates=[\"date\"],\n",
    "            dtype={\n",
    "                \"store_nbr\": str,\n",
    "            },\n",
    "        )\n",
    "        .pipe(lambda df: df.assign(family=df.loc[:, \"family\"].str.replace(\"/\", \" OR \")))\n",
    "        .set_index([\"store_nbr\", \"family\", \"date\"])\n",
    "        .reindex(_TEST_DATETIME_INDEX, level=\"date\")\n",
    "        .sort_index(axis=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_transactions_df(filepath=\"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\") -> pd.DataFrame:\n",
    "\n",
    "    df = (\n",
    "        pd.read_csv(\n",
    "            filepath,\n",
    "            parse_dates=[\"date\"],\n",
    "            dtype={\n",
    "                \"store_nbr\": str,\n",
    "                \"transactions\": np.float32,\n",
    "            },\n",
    "        )\n",
    "        .set_index([\"store_nbr\", \"date\"])\n",
    "        .reindex(_TRAIN_DATETIME_INDEX, level=\"date\")\n",
    "        .sort_index(axis=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_oil_df(filepath=\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\") -> pd.DataFrame:\n",
    "\n",
    "    df = (\n",
    "        pd.read_csv(\n",
    "            filepath,\n",
    "            parse_dates=[\"date\"],\n",
    "            dtype={\n",
    "                \"dcoilwtico\": np.float32,\n",
    "            },\n",
    "        )\n",
    "        .set_index(\"date\")\n",
    "        .reindex(_COMBINED_DATETIME_INDEX, level=\"date\")\n",
    "        .sort_index(axis=0)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_stores_df(filepath=\"/kaggle/input/store-sales-time-series-forecasting/stores.csv\") -> pd.DataFrame:\n",
    "\n",
    "    df = (\n",
    "        pd.read_csv(\n",
    "            filepath,\n",
    "            index_col=[\"store_nbr\"],\n",
    "            dtype={\n",
    "                \"store_nbr\": str,\n",
    "                \"cluster\": str,\n",
    "            },\n",
    "        )\n",
    "        .rename(columns={\"type\": \"store_type\", \"cluster\": \"store_cluster\"})\n",
    "        .assign(country=\"Ecuador\")\n",
    "        .sort_index(axis=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_holidays_events_df(\n",
    "    filepath=\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\",\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df = (\n",
    "        pd.read_csv(\n",
    "            filepath,\n",
    "            parse_dates=[\"date\"],\n",
    "        )\n",
    "        .replace({\"2013-04-29\": pd.to_datetime(\"2013-03-29\")})  # 'Good Friday' mistake correction\n",
    "        .sort_index(axis=0)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for cleaning the training data\n",
    "\n",
    "The quality of the data prior to mid-2015 is poor for many of the data series so I decided to drop it; many of the remaining series have significant numbers of leading zero values for sales. These leading zero sales values are better coded as missing values. I replace all the leading zero values for sales and then drop all the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T05:49:48.046228Z",
     "iopub.status.busy": "2023-10-18T05:49:48.045875Z",
     "iopub.status.idle": "2023-10-18T05:49:48.05605Z",
     "shell.execute_reply": "2023-10-18T05:49:48.054929Z",
     "shell.execute_reply.started": "2023-10-18T05:49:48.046201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _all_false_mask(df):\n",
    "    mask_df = pd.DataFrame(False, columns=df.columns, index=df.index)\n",
    "    return mask_df\n",
    "\n",
    "\n",
    "def _create_leading_zero_sales_mask(df):\n",
    "    mask_df = _all_false_mask(df)\n",
    "    sales_all_zero = df.loc[:, \"sales\"].eq(0).all()\n",
    "    if not sales_all_zero:\n",
    "        mask_df[\"sales\"] = df.loc[:, \"sales\"].eq(0).cummin()\n",
    "    return mask_df\n",
    "\n",
    "\n",
    "def _drop_leading_zero_sales(df):\n",
    "    masked_df = _mask_leading_zero_sales(df)\n",
    "    without_leading_zeros_df = masked_df.dropna(how=\"any\", subset=[\"sales\"], axis=0)\n",
    "    return without_leading_zeros_df\n",
    "\n",
    "\n",
    "def _mask_leading_zero_sales(df):\n",
    "    cond = df.groupby([\"store_nbr\", \"family\"], group_keys=False).apply(lambda df: _create_leading_zero_sales_mask(df))\n",
    "    masked_df = df.mask(cond, np.nan)\n",
    "    return masked_df\n",
    "\n",
    "\n",
    "def _use_only_recent_data(df, start):\n",
    "    recent_df = df.loc[pd.IndexSlice[:, :, start:]]\n",
    "    return recent_df\n",
    "\n",
    "\n",
    "def make_data_cleaning_pipeline(start=\"2015-07-01\", verbose=True) -> Pipeline:\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (f\"Use only data after {start}\", FunctionTransformer(func=_use_only_recent_data, kw_args={\"start\": start})),\n",
    "            (\n",
    "                \"Drop leading zero sales values\",\n",
    "                FunctionTransformer(\n",
    "                    func=_drop_leading_zero_sales,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        verbose=verbose,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for joining the data sets\n",
    "\n",
    "After cleaning the training data, I join all the `test.csv`, `stores.csv`, `oil.csv`, and `transactions.csv`. I leave the `holidays_events.csv` alone for now as I will add it later during the feature engineering step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:51:13.649849Z",
     "iopub.status.busy": "2023-10-12T04:51:13.649348Z",
     "iopub.status.idle": "2023-10-12T04:51:13.676088Z",
     "shell.execute_reply": "2023-10-12T04:51:13.675253Z",
     "shell.execute_reply.started": "2023-10-12T04:51:13.649808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _load_and_join_oil_df(df, filepath):\n",
    "    oil_df = load_oil_df(filepath)\n",
    "    joined_df = df.join(oil_df, how=\"left\", on=[\"date\"]).sort_index(axis=0).sort_index(axis=1)\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "def _load_and_join_stores_df(df, filepath):\n",
    "    stores_df = load_stores_df(filepath)\n",
    "    joined_df = df.join(stores_df, how=\"left\", on=[\"store_nbr\"]).sort_index(axis=0).sort_index(axis=1)\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "def _load_and_concat_test_df(df, filepath, onpromotion):\n",
    "    test_df = load_test_df(filepath, onpromotion)\n",
    "    concat_df = pd.concat(\n",
    "        [df, test_df],\n",
    "        axis=0,\n",
    "        sort=True,\n",
    "    ).sort_index(axis=0)\n",
    "    return concat_df\n",
    "\n",
    "\n",
    "def _load_and_join_transactions_df(df, filepath):\n",
    "    transactions_df = load_transactions_df(filepath)\n",
    "    joined_df = df.join(transactions_df, how=\"left\", on=[\"store_nbr\", \"date\"]).sort_index(axis=0).sort_index(axis=1)\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "def _make_oil_joining_pipeline(filepath, verbose):\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"Load and join the data\", FunctionTransformer(func=_load_and_join_oil_df, kw_args={\"filepath\": filepath})),\n",
    "            (\n",
    "                \"Impute missing values\",\n",
    "                ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\n",
    "                            \"Forward fill missing dcoilwtico values\",\n",
    "                            FunctionTransformer(lambda df: df.fillna(method=\"ffill\")),\n",
    "                            [\"dcoilwtico\"],\n",
    "                        ),\n",
    "                    ],\n",
    "                    n_jobs=1,\n",
    "                    remainder=\"passthrough\",\n",
    "                    verbose=verbose,\n",
    "                    verbose_feature_names_out=False,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def _make_transactions_joining_pipeline(filepath, verbose):\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"Load and join the data\",\n",
    "                FunctionTransformer(func=_load_and_join_transactions_df, kw_args={\"filepath\": filepath}),\n",
    "            ),\n",
    "            (\n",
    "                \"Impute missing values\",\n",
    "                ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\n",
    "                            \"Fill missing transactions with zeros\",\n",
    "                            SimpleImputer(strategy=\"constant\", fill_value=0.0),\n",
    "                            [\"transactions\"],\n",
    "                        )\n",
    "                    ],\n",
    "                    n_jobs=1,\n",
    "                    remainder=\"passthrough\",\n",
    "                    verbose=verbose,\n",
    "                    verbose_feature_names_out=False,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def _make_data_joining_pipeline(\n",
    "    use_future_onpromotion, use_future_dcoilwtico, concat_test, join_oil, join_stores, join_transactions, verbose\n",
    "):\n",
    "\n",
    "    steps = []\n",
    "    if join_transactions:\n",
    "        _pipeline = _make_transactions_joining_pipeline(\n",
    "            \"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\", verbose\n",
    "        )\n",
    "        steps.append((\"Load and join the transactions data\", _pipeline))\n",
    "\n",
    "    # if joining oil and also concatenating the test data, pipeline steps\n",
    "    # order depends on whether you wish to use future dcoilwtico data.\n",
    "    if join_oil and concat_test:\n",
    "        _pipeline = _make_oil_joining_pipeline(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\", verbose)\n",
    "\n",
    "        if use_future_dcoilwtico:\n",
    "            steps.extend(\n",
    "                [\n",
    "                    (\n",
    "                        \"Load and concatenate the test data\",\n",
    "                        FunctionTransformer(\n",
    "                            func=_load_and_concat_test_df,\n",
    "                            kw_args={\n",
    "                                \"filepath\": \"/kaggle/input/store-sales-time-series-forecasting/test.csv\",\n",
    "                                \"onpromotion\": use_future_onpromotion,\n",
    "                            },\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\"Load and join the oil price data\", _pipeline),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            steps.extend(\n",
    "                [\n",
    "                    (\"Load and join the oil price data\", _pipeline),\n",
    "                    (\n",
    "                        \"Load and concatenate the test data\",\n",
    "                        FunctionTransformer(\n",
    "                            func=_load_and_concat_test_df,\n",
    "                            kw_args={\n",
    "                                \"filepath\": \"/kaggle/input/store-sales-time-series-forecasting/test.csv\",\n",
    "                                \"onpromotion\": use_future_onpromotion,\n",
    "                            },\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "    elif join_oil and not concat_test:\n",
    "        steps.append(\n",
    "            (\"Load and join the oil price data\", _pipeline),\n",
    "        )\n",
    "    elif not join_oil and concat_test:\n",
    "        steps.append(\n",
    "            (\n",
    "                \"Load and concatenate the test data\",\n",
    "                FunctionTransformer(\n",
    "                    func=_load_and_concat_test_df,\n",
    "                    kw_args={\n",
    "                        \"filepath\": \"/kaggle/input/store-sales-time-series-forecasting/test.csv\",\n",
    "                        \"onpromotion\": use_future_onpromotion,\n",
    "                    },\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    # if not joining oil and not concatenating test data, then just move on\n",
    "    # to joining the stores data.\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if join_stores:\n",
    "        steps.append(\n",
    "            (\n",
    "                \"Load and join the stores data\",\n",
    "                FunctionTransformer(\n",
    "                    func=_load_and_join_stores_df,\n",
    "                    kw_args={\"filepath\": \"/kaggle/input/store-sales-time-series-forecasting/stores.csv\"},\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    pipeline = Pipeline(steps, verbose=verbose).set_output(transform=\"pandas\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full preprocessing pipeline\n",
    "\n",
    "The full preprocessing pipeline combines the cleaning steps, the joining steps, with a final step which clips outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:51:13.678069Z",
     "iopub.status.busy": "2023-10-12T04:51:13.677092Z",
     "iopub.status.idle": "2023-10-12T04:51:13.700885Z",
     "shell.execute_reply": "2023-10-12T04:51:13.699739Z",
     "shell.execute_reply.started": "2023-10-12T04:51:13.678041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _clip_outliers(df, q):\n",
    "    expanding_quantiles_df = df.groupby(level=[\"store_nbr\", \"family\"], group_keys=False).apply(\n",
    "        lambda _: _compute_expanding_quantiles(_, q)\n",
    "    )\n",
    "    mask = _all_false_mask(df)\n",
    "    is_outlier = df.gt(expanding_quantiles_df).to_dict()\n",
    "    cond = mask.assign(**is_outlier)\n",
    "    clipped_df = df.mask(cond, expanding_quantiles_df)\n",
    "    return clipped_df\n",
    "\n",
    "\n",
    "def _compute_expanding_quantiles(df, q):\n",
    "    expanding_quantiles = df.expanding(min_periods=1).quantile(q).to_dict()\n",
    "    expanding_quantile_df = df.assign(**expanding_quantiles).astype(np.float32)\n",
    "    return expanding_quantile_df\n",
    "\n",
    "\n",
    "def _make_clip_outliers_transformer(onpromotion, dcoilwtico, transactions, q, n_jobs, verbose):\n",
    "\n",
    "    transformers = [(\"Clip sales outliers\", FunctionTransformer(func=_clip_outliers, kw_args={\"q\": q}), [\"sales\"])]\n",
    "\n",
    "    if onpromotion:\n",
    "        transformers.append(\n",
    "            (\"Clip onpromotion outliers\", FunctionTransformer(func=_clip_outliers, kw_args={\"q\": q}), [\"onpromotion\"])\n",
    "        )\n",
    "\n",
    "    if dcoilwtico:\n",
    "        transformers.append(\n",
    "            (\"Clip dcoilwtico outliers\", FunctionTransformer(func=_clip_outliers, kw_args={\"q\": q}), [\"dcoilwtico\"])\n",
    "        )\n",
    "\n",
    "    if transactions:\n",
    "        transformers.append(\n",
    "            (\"Clip transactions outliers\", FunctionTransformer(func=_clip_outliers, kw_args={\"q\": q}), [\"transactions\"])\n",
    "        )\n",
    "\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers, n_jobs=n_jobs, remainder=\"passthrough\", verbose=verbose, verbose_feature_names_out=False\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def make_data_preprocessing_pipeline(\n",
    "    use_future_onpromotion=False,\n",
    "    use_future_dcoilwtico=False,\n",
    "    concat_test=True,\n",
    "    join_oil=True,\n",
    "    join_stores=True,\n",
    "    join_transactions=True,\n",
    "    start=\"20150701\",\n",
    "    q=0.99,\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    ") -> Pipeline:\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"Clean the training data\", make_data_cleaning_pipeline(start, verbose)),\n",
    "            (\n",
    "                \"Join the additional data sets\",\n",
    "                _make_data_joining_pipeline(\n",
    "                    use_future_onpromotion,\n",
    "                    use_future_dcoilwtico,\n",
    "                    concat_test,\n",
    "                    join_oil,\n",
    "                    join_stores,\n",
    "                    join_transactions,\n",
    "                    verbose,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"Clip outliers\",\n",
    "                _make_clip_outliers_transformer(\n",
    "                    use_future_onpromotion, join_oil, join_transactions, q, n_jobs, verbose\n",
    "                ),\n",
    "            ),\n",
    "            (\"Sort the columns\", FunctionTransformer(func=lambda df: df.sort_index(axis=1))),\n",
    "        ],\n",
    "        verbose=verbose,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Un-comment and run the code below to see the data preprocessing pipeline in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:51:13.703812Z",
     "iopub.status.busy": "2023-10-12T04:51:13.7033Z",
     "iopub.status.idle": "2023-10-12T04:51:13.725028Z",
     "shell.execute_reply": "2023-10-12T04:51:13.723996Z",
     "shell.execute_reply.started": "2023-10-12T04:51:13.703744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _train_df = load_train_df(\n",
    "#    \"/kaggle/input/store-sales-time-series-forecasting/train.csv\",\n",
    "#    onpromotion=True,\n",
    "# )\n",
    "# _pipeline = make_data_preprocessing_pipeline(\n",
    "#    use_future_onpromotion=True,\n",
    "#    use_future_dcoilwtico=True,\n",
    "# )\n",
    "# _preprocessed_df = _pipeline.fit_transform(_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the future values for `dcoilwtico`, `onpromotion`, and `transactions` would not be available at the time that we need to generate our sales forecasts in a real forecasting setting. For the purposes of the competition we are given the values for `dcoilwtico` and `onpromotion` for the test window. We are not given the `transactions` values and will need to forecast them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:51:13.726897Z",
     "iopub.status.busy": "2023-10-12T04:51:13.726523Z",
     "iopub.status.idle": "2023-10-12T04:51:13.738544Z",
     "shell.execute_reply": "2023-10-12T04:51:13.737368Z",
     "shell.execute_reply.started": "2023-10-12T04:51:13.726869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:04.84301Z",
     "iopub.status.busy": "2023-10-12T04:58:04.842363Z",
     "iopub.status.idle": "2023-10-12T04:58:04.850892Z",
     "shell.execute_reply": "2023-10-12T04:58:04.849719Z",
     "shell.execute_reply.started": "2023-10-12T04:58:04.84298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _create_date_features(df):\n",
    "    _df = df.reset_index(\"date\")\n",
    "    date_features = {\n",
    "        \"month\": _df.loc[:, \"date\"].dt.month.astype(np.uint8),\n",
    "        \"day_of_month\": _df.loc[:, \"date\"].dt.day.astype(np.uint8),\n",
    "        \"day_of_week\": _df.loc[:, \"date\"].dt.day_of_week.astype(np.uint8),\n",
    "        \"is_month_start\": _df.loc[:, \"date\"].dt.day.eq(1).astype(np.uint8),\n",
    "        \"is_month_middle\": _df.loc[:, \"date\"].dt.day.eq(15).astype(np.uint8),\n",
    "        # \"is_month_end\": _df.loc[:, \"date\"].dt.day.eq(.astype(np.uint8),\n",
    "        \"is_weekend\": _df.loc[:, \"date\"].dt.day_of_week.floordiv(5).astype(np.uint8),\n",
    "    }\n",
    "    with_date_features_df = (\n",
    "        _df.assign(**date_features).set_index(\"date\", append=True).sort_index(axis=0).sort_index(axis=1)\n",
    "    )\n",
    "    return with_date_features_df\n",
    "\n",
    "\n",
    "def make_date_features_transformer(verbose=True):\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"Engineer date features\",\n",
    "                FunctionTransformer(\n",
    "                    func=_create_date_features,\n",
    "                ),\n",
    "                [\"id\"],\n",
    "            )\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose=verbose,\n",
    "        verbose_feature_names_out=False,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.168007Z",
     "iopub.status.busy": "2023-10-12T04:58:14.167278Z",
     "iopub.status.idle": "2023-10-12T04:58:14.172358Z",
     "shell.execute_reply": "2023-10-12T04:58:14.171205Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.167975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _transformer = make_date_features_transformer()\n",
    "# _transformer.fit_transform(_preprocessed_df).groupby(\"month\")[\"day_of_month\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays and Events Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make heavy use of Pandas string processing methods to extract a unique description for each holiday at the national, regional, and local level. I will use these descriptions to create indicator/dummy variables and add them as features in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.209944Z",
     "iopub.status.busy": "2023-10-12T04:58:14.209279Z",
     "iopub.status.idle": "2023-10-12T04:58:14.25331Z",
     "shell.execute_reply": "2023-10-12T04:58:14.252204Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.209917Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m _HOLIDAYS_EVENTS_DF = \u001b[43mload_holidays_events_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m HOLIDAY_TYPES = [\u001b[33m\"\u001b[39m\u001b[33mAdditional\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBridge\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEvent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHoliday\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTransfer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWork Day\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m LOCAL_HOLIDAY_DESCRIPTIONS = (\n\u001b[32m     10\u001b[39m     _HOLIDAYS_EVENTS_DF.query(\u001b[33m\"\u001b[39m\u001b[33m(locale == \u001b[39m\u001b[33m'\u001b[39m\u001b[33mLocal\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) & (type in @HOLIDAY_TYPES) & (~transferred)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m                        .loc[:, \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m                        .tolist() \n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mload_holidays_events_df\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_holidays_events_df\u001b[39m(filepath=\u001b[33m\"\u001b[39m\u001b[33m/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\u001b[39m\u001b[33m\"\u001b[39m) -> pd.DataFrame:\n\u001b[32m    139\u001b[39m     df = (\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.replace({\u001b[33m\"\u001b[39m\u001b[33m2013-04-29\u001b[39m\u001b[33m\"\u001b[39m: pd.to_datetime(\u001b[33m\"\u001b[39m\u001b[33m2013-03-29\u001b[39m\u001b[33m\"\u001b[39m)}) \u001b[38;5;66;03m# 'Good Friday' mistake correction\u001b[39;00m\n\u001b[32m    144\u001b[39m          .sort_index(axis=\u001b[32m0\u001b[39m)\n\u001b[32m    145\u001b[39m          .sort_index(axis=\u001b[32m1\u001b[39m)\n\u001b[32m    146\u001b[39m     )\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/kret_312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/kret_312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/kret_312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/kret_312/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/kret_312/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv'"
     ]
    }
   ],
   "source": [
    "_HOLIDAYS_EVENTS_DF = load_holidays_events_df(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\")\n",
    "\n",
    "\n",
    "HOLIDAY_TYPES = [\"Additional\", \"Bridge\", \"Event\", \"Holiday\", \"Transfer\", \"Work Day\"]\n",
    "\n",
    "\n",
    "LOCAL_HOLIDAY_DESCRIPTIONS = (\n",
    "    _HOLIDAYS_EVENTS_DF.query(\"(locale == 'Local') & (type in @HOLIDAY_TYPES) & (~transferred)\")\n",
    "    .loc[:, \"description\"]\n",
    "    .str.removeprefix(\"Traslado \")\n",
    "    .str.strip(\"-+0123456789\")\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "NATIONAL_HOLIDAY_DESCRIPTIONS = (\n",
    "    _HOLIDAYS_EVENTS_DF.query(\"(locale == 'National') & (type in @HOLIDAY_TYPES) & (~transferred)\")\n",
    "    .loc[:, \"description\"]\n",
    "    .str.removeprefix(\"Traslado \")\n",
    "    .str.removeprefix(\"Puente \")\n",
    "    .str.removeprefix(\"Inauguracion \")\n",
    "    .str.replace(\":.*\", \"\", regex=True)\n",
    "    .str.replace(\"puente\", \"Puente\")\n",
    "    .str.replace(\"primer\", \"Primer\")\n",
    "    .str.strip(\"-+0123456789\")\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "\n",
    "REGIONAL_HOLIDAY_DESCRIPTIONS = (\n",
    "    _HOLIDAYS_EVENTS_DF.query(\"(locale == 'Regional') & (type in @HOLIDAY_TYPES) & (~transferred)\")\n",
    "    .loc[:, \"description\"]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define some functions which will use the unique descriptions and create an indicator for each individual holiday. I need separate functions for the different the locales as the joining logic is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.280554Z",
     "iopub.status.busy": "2023-10-12T04:58:14.280197Z",
     "iopub.status.idle": "2023-10-12T04:58:14.292686Z",
     "shell.execute_reply": "2023-10-12T04:58:14.291923Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.280528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _create_some_local_holiday_indicator(df, description):\n",
    "    is_local_holiday = (\n",
    "        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Local') & (type in {HOLIDAY_TYPES}) & (~transferred)\")\n",
    "        .loc[:, \"description\"]\n",
    "        .str.contains(description, case=False)\n",
    "    )\n",
    "    indicator_name = f\"is_{description.lower().replace(' ', '_')}\"\n",
    "    is_local_holiday_df = (\n",
    "        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Local') & (type in {HOLIDAY_TYPES}) & (~transferred)\")\n",
    "        .loc[is_local_holiday, [\"date\", \"locale_name\"]]\n",
    "        .rename(columns={\"locale_name\": \"city\"})\n",
    "        .assign(**{indicator_name: 1})\n",
    "        .drop_duplicates()\n",
    "        .set_index([\"date\", \"city\"])\n",
    "    )\n",
    "    with_local_holiday_indicator_df = (\n",
    "        df.join(is_local_holiday_df, how=\"left\", on=[\"date\", \"city\"])\n",
    "        .fillna(value={indicator_name: 0})\n",
    "        .astype({indicator_name: np.uint8})\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    return with_local_holiday_indicator_df\n",
    "\n",
    "\n",
    "def _create_some_national_holiday_indicator(df, description):\n",
    "    is_national_holiday = (\n",
    "        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'National') & (type in {HOLIDAY_TYPES}) & (~transferred)\")\n",
    "        .loc[:, \"description\"]\n",
    "        .str.contains(description, case=False)\n",
    "    )\n",
    "    indicator_name = f\"is_{description.lower().replace(' ', '_')}\"\n",
    "    is_national_holiday_df = (\n",
    "        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'National') & (type in {HOLIDAY_TYPES}) & (~transferred)\")\n",
    "        .loc[is_national_holiday, [\"date\"]]\n",
    "        .assign(**{indicator_name: 1})\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"date\")\n",
    "    )\n",
    "    with_national_holiday_indicator_df = (\n",
    "        df.join(is_national_holiday_df, how=\"left\", on=[\"date\"])\n",
    "        .fillna(value={indicator_name: 0})\n",
    "        .astype({indicator_name: np.uint8})\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    return with_national_holiday_indicator_df\n",
    "\n",
    "\n",
    "def _create_some_regional_holiday_indicator(df, description):\n",
    "    is_regional_holiday = (\n",
    "        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Regional') & (type in {HOLIDAY_TYPES}) & (~transferred)\")\n",
    "        .loc[:, \"description\"]\n",
    "        .str.contains(description, case=False)\n",
    "    )\n",
    "    indicator_name = f\"is_{description.lower().replace(' ', '_')}\"\n",
    "    is_regional_holiday_df = (\n",
    "        _HOLIDAYS_EVENTS_DF.query(f\"(locale == 'Regional') & (type in {HOLIDAY_TYPES}) & (~transferred)\")\n",
    "        .loc[is_regional_holiday, [\"date\", \"locale_name\"]]\n",
    "        .rename(columns={\"locale_name\": \"state\"})\n",
    "        .assign(**{indicator_name: 1})\n",
    "        .drop_duplicates()\n",
    "        .set_index([\"date\", \"state\"])\n",
    "    )\n",
    "    with_regional_holiday_indicator_df = (\n",
    "        df.join(is_regional_holiday_df, how=\"left\", on=[\"date\", \"state\"])\n",
    "        .fillna(value={indicator_name: 0})\n",
    "        .astype({indicator_name: np.uint8})\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    return with_regional_holiday_indicator_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I define functions that build column transformers for each locale's holidays and events. Using column transformers allows my to parallelize the feature creation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.358072Z",
     "iopub.status.busy": "2023-10-12T04:58:14.357667Z",
     "iopub.status.idle": "2023-10-12T04:58:14.367516Z",
     "shell.execute_reply": "2023-10-12T04:58:14.36665Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.358044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _make_local_holidays_transformer(n_jobs=-1, verbose=True):\n",
    "    transformers = []\n",
    "    for description in LOCAL_HOLIDAY_DESCRIPTIONS:\n",
    "        transformers.append(\n",
    "            (\n",
    "                f\"Create {description} indicator\",\n",
    "                FunctionTransformer(func=_create_some_local_holiday_indicator, kw_args={\"description\": description}),\n",
    "                [\"city\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers, n_jobs=n_jobs, remainder=\"drop\", verbose=verbose, verbose_feature_names_out=False\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def _make_national_holidays_transformer(n_jobs=-1, verbose=True):\n",
    "    transformers = []\n",
    "    for description in NATIONAL_HOLIDAY_DESCRIPTIONS:\n",
    "        transformers.append(\n",
    "            (\n",
    "                f\"Create {description} indicator\",\n",
    "                FunctionTransformer(func=_create_some_national_holiday_indicator, kw_args={\"description\": description}),\n",
    "                [\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers, n_jobs=n_jobs, remainder=\"drop\", verbose=verbose, verbose_feature_names_out=False\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def _make_regional_holidays_transformer(n_jobs=-1, verbose=True):\n",
    "    transformers = []\n",
    "    for description in REGIONAL_HOLIDAY_DESCRIPTIONS:\n",
    "        transformers.append(\n",
    "            (\n",
    "                f\"Create {description} indicator\",\n",
    "                FunctionTransformer(func=_create_some_regional_holiday_indicator, kw_args={\"description\": description}),\n",
    "                [\"state\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers, n_jobs=n_jobs, remainder=\"drop\", verbose=verbose, verbose_feature_names_out=False\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I use a `FeatureUnion` to join together the three `ColumnTransformer` objects. This adds a further level of parallelism because now each of these three transformers can be computed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.471454Z",
     "iopub.status.busy": "2023-10-12T04:58:14.470935Z",
     "iopub.status.idle": "2023-10-12T04:58:14.481664Z",
     "shell.execute_reply": "2023-10-12T04:58:14.480896Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.471427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _create_local_holiday_indicator(df):\n",
    "    indicator = df.select_dtypes(include=np.uint8).max(axis=1)\n",
    "    with_indicator_df = df.assign(is_local_holiday=indicator)\n",
    "    return with_indicator_df\n",
    "\n",
    "\n",
    "def _create_national_holiday_indicator(df):\n",
    "    indicator = df.select_dtypes(include=np.uint8).max(axis=1)\n",
    "    with_indicator_df = df.assign(is_national_holiday=indicator)\n",
    "    return with_indicator_df\n",
    "\n",
    "\n",
    "def _create_regional_holiday_indicator(df):\n",
    "    indicator = df.select_dtypes(include=np.uint8).max(axis=1)\n",
    "    with_indicator_df = df.assign(is_regional_holiday=indicator)\n",
    "    return with_indicator_df\n",
    "\n",
    "\n",
    "def _remove_duplicated_columns(df):\n",
    "    return df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "\n",
    "def make_holidays_events_feature_union(n_jobs=-1, verbose=True):\n",
    "    feature_union = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            (\n",
    "                \"Engineer local holiday features\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\n",
    "                            \"Engineer individual local holiday indicators\",\n",
    "                            _make_local_holidays_transformer(\n",
    "                                n_jobs=-1,\n",
    "                                verbose=verbose,\n",
    "                            ),\n",
    "                        ),\n",
    "                        (\"Remove any duplicated columns\", FunctionTransformer(func=_remove_duplicated_columns)),\n",
    "                        (\"Engineer local holiday indicator\", FunctionTransformer(func=_create_local_holiday_indicator)),\n",
    "                    ],\n",
    "                    verbose=verbose,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"Engineer national holiday and event features\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\n",
    "                            \"Engineer indivdual national holiday and event indicators\",\n",
    "                            _make_national_holidays_transformer(n_jobs=-1, verbose=verbose),\n",
    "                        ),\n",
    "                        (\"Remove any duplicated columns\", FunctionTransformer(func=_remove_duplicated_columns)),\n",
    "                        (\n",
    "                            \"Engineer national holiday and event indicator\",\n",
    "                            FunctionTransformer(func=_create_national_holiday_indicator),\n",
    "                        ),\n",
    "                    ],\n",
    "                    verbose=verbose,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"Engineer regional holiday features\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\n",
    "                            \"Engineer individual regional holiday indicators\",\n",
    "                            _make_regional_holidays_transformer(n_jobs=-1, verbose=verbose),\n",
    "                        ),\n",
    "                        (\"Remove any duplicated columns\", FunctionTransformer(func=_remove_duplicated_columns)),\n",
    "                        (\n",
    "                            \"Engineer regional holiday indicator\",\n",
    "                            FunctionTransformer(func=_create_regional_holiday_indicator),\n",
    "                        ),\n",
    "                    ],\n",
    "                    verbose=verbose,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return feature_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "\n",
    "Un-comment and run the code in the cell below to see the pipeline in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.504862Z",
     "iopub.status.busy": "2023-10-12T04:58:14.504467Z",
     "iopub.status.idle": "2023-10-12T04:58:14.508864Z",
     "shell.execute_reply": "2023-10-12T04:58:14.508073Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.504835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _feature_union = make_holidays_events_feature_union(\n",
    "#    n_jobs=-1,\n",
    "#    verbose=True\n",
    "# )\n",
    "# _df = _feature_union.fit_transform(_preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagged Feature Engineering Pipeline\n",
    "\n",
    "Notice that in order to properly create the lagged features from the transactions data that we will need to train our model and then forecast sales, we will need a model to forecast the missing transactions over the test window. Typically we would also need models for other features like `dcoilwtico` and `onpromotion` (and any other features we use during training that are not known at the time we make our forecast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.527197Z",
     "iopub.status.busy": "2023-10-12T04:58:14.525961Z",
     "iopub.status.idle": "2023-10-12T04:58:14.535349Z",
     "shell.execute_reply": "2023-10-12T04:58:14.534244Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.527155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _totals_by_store_nbr(df, onpromotion, transactions):\n",
    "    features = [\"sales\"]\n",
    "    if onpromotion:\n",
    "        features.append(\"onpromotion\")\n",
    "    if transactions:\n",
    "        features.append(transactions)\n",
    "    totals_df = (\n",
    "        df.loc[:, features]\n",
    "        .groupby([\"store_nbr\", \"date\"])\n",
    "        .sum()\n",
    "        .rename(columns={feature: f\"total_{feature}\" for feature in features})\n",
    "    )\n",
    "    return totals_df\n",
    "\n",
    "\n",
    "def _sales_per_transaction(df):\n",
    "    totals_df = _totals_by_store_nbr(df, onpromotion=False, transactions=True)\n",
    "    sales_per_transaction = totals_df.loc[:, \"sales\"].div(totals_df.loc[:, \"transactions\"])\n",
    "    return sales_per_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.586691Z",
     "iopub.status.busy": "2023-10-12T04:58:14.586304Z",
     "iopub.status.idle": "2023-10-12T04:58:14.605365Z",
     "shell.execute_reply": "2023-10-12T04:58:14.604168Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.586665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _create_lagged_features(s, lags, drop):\n",
    "    feature = s.name\n",
    "    lagged_features = {}\n",
    "    for lag in lags:\n",
    "        lagged_features[f\"{feature}_lag{lag:02d}\"] = (\n",
    "            s.groupby([\"store_nbr\", \"family\"]).shift(periods=lag).fillna(method=\"bfill\").astype(np.float32)\n",
    "        )\n",
    "    if drop:\n",
    "        with_lagged_features_df = s.to_frame().assign(**lagged_features).drop(feature, axis=1).sort_index(axis=1)\n",
    "    else:\n",
    "        with_lagged_features_df = s.to_frame().assign(**lagged_features).sort_index(axis=1)\n",
    "    return with_lagged_features_df\n",
    "\n",
    "\n",
    "def _create_rolling_mean(s, lag, window):\n",
    "    rolling_mean = {\n",
    "        f\"{s.name}_{window:02d}_day_rolling_mean\": (\n",
    "            s.groupby([\"store_nbr\", \"family\"])\n",
    "            .rolling(min_periods=1, window=window)\n",
    "            .mean()\n",
    "            .astype(np.float32)\n",
    "            .droplevel([0, 1])\n",
    "        )\n",
    "    }\n",
    "    with_rolling_mean_df = s.to_frame().assign(**rolling_mean).sort_index(axis=1)\n",
    "    return with_rolling_mean_df\n",
    "\n",
    "\n",
    "def _fillna_with_forecast(df, forecast_df):\n",
    "    with_forecast_df = df.fillna(forecast_df)\n",
    "    return with_forecast_df\n",
    "\n",
    "\n",
    "def _make_fillna_with_forecast_transformer(\n",
    "    dcoilwtico_forecast_df, onpromotion_forecast_df, transactions_forecast_df, n_jobs, verbose\n",
    "):\n",
    "\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"Insert dcoilwtico forecast\",\n",
    "                FunctionTransformer(func=_fillna_with_forecast, kw_args={\"forecast_df\": dcoilwtico_forecast_df}),\n",
    "                [\"dcoilwtico\"],\n",
    "            ),\n",
    "            (\n",
    "                \"Insert onpromotion forecast\",\n",
    "                FunctionTransformer(func=_fillna_with_forecast, kw_args={\"forecast_df\": onpromotion_forecast_df}),\n",
    "                [\"onpromotion\"],\n",
    "            ),\n",
    "            (\n",
    "                \"Insert transactions forecast\",\n",
    "                FunctionTransformer(func=_fillna_with_forecast, kw_args={\"forecast_df\": transactions_forecast_df}),\n",
    "                [\"transactions\"],\n",
    "            ),\n",
    "        ],\n",
    "        n_jobs=n_jobs,\n",
    "        remainder=\"drop\",\n",
    "        verbose=verbose,\n",
    "        verbose_feature_names_out=False,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def _make_lagged_feature_transformer(max_lags, n_jobs, verbose):\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"Engineer lagged dcoilwtico features\",\n",
    "                FunctionTransformer(\n",
    "                    func=_create_lagged_features,\n",
    "                    kw_args={\n",
    "                        \"lags\": range(1, max_lags + 1),\n",
    "                        \"drop\": True,\n",
    "                    },\n",
    "                ),\n",
    "                \"dcoilwtico\",\n",
    "            ),\n",
    "            (\n",
    "                \"Engineer lagged onpromotion features\",\n",
    "                FunctionTransformer(\n",
    "                    func=_create_lagged_features,\n",
    "                    kw_args={\n",
    "                        \"lags\": range(1, max_lags + 1),\n",
    "                        \"drop\": True,\n",
    "                    },\n",
    "                ),\n",
    "                \"onpromotion\",\n",
    "            ),\n",
    "            (\n",
    "                \"Engineer lagged transactions features\",\n",
    "                FunctionTransformer(\n",
    "                    func=_create_lagged_features,\n",
    "                    kw_args={\n",
    "                        \"lags\": range(1, max_lags + 1),\n",
    "                        \"drop\": True,\n",
    "                    },\n",
    "                ),\n",
    "                \"transactions\",\n",
    "            ),\n",
    "        ],\n",
    "        n_jobs=n_jobs,\n",
    "        remainder=\"drop\",\n",
    "        verbose=verbose,\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def _make_rolling_means_transformer(lag, window, n_jobs, verbose) -> ColumnTransformer:\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                f\"Engineer dcoilwtico_lag{lag:02d} {window:02d}-day rolling mean\",\n",
    "                FunctionTransformer(func=_create_rolling_mean, kw_args={\"lag\": lag, \"window\": window}),\n",
    "                f\"dcoilwtico_lag{lag:02d}\",\n",
    "            ),\n",
    "            (\n",
    "                f\"Engineer onpromotion_lag{lag:02d} {window:02d}-day rolling mean\",\n",
    "                FunctionTransformer(func=_create_rolling_mean, kw_args={\"lag\": lag, \"window\": window}),\n",
    "                f\"onpromotion_lag{lag:02d}\",\n",
    "            ),\n",
    "            (\n",
    "                f\"Engineer transactions_lag{lag:02d} {window:02d}-day rolling mean\",\n",
    "                FunctionTransformer(func=_create_rolling_mean, kw_args={\"lag\": lag, \"window\": window}),\n",
    "                f\"transactions_lag{lag:02d}\",\n",
    "            ),\n",
    "        ],\n",
    "        n_jobs=n_jobs,\n",
    "        remainder=\"passthrough\",\n",
    "        verbose=verbose,\n",
    "        verbose_feature_names_out=False,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def make_lagged_feature_engineering_pipeline(\n",
    "    max_lags, dcoilwtico_forecast_df, onpromotion_forecast_df, transactions_forecast_df, n_jobs=-1, verbose=True\n",
    ") -> Pipeline:\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"Insert test window forecasts\",\n",
    "                _make_fillna_with_forecast_transformer(\n",
    "                    dcoilwtico_forecast_df,\n",
    "                    onpromotion_forecast_df,\n",
    "                    transactions_forecast_df,\n",
    "                    n_jobs,\n",
    "                    verbose,\n",
    "                ),\n",
    "            ),\n",
    "            (\"Create lagged features\", _make_lagged_feature_transformer(max_lags, n_jobs, verbose)),\n",
    "            (\"Create weekly rolling means\", _make_rolling_means_transformer(1, 7, n_jobs, verbose)),\n",
    "            (\"Create monthly rolling means\", _make_rolling_means_transformer(1, 28, n_jobs, verbose)),\n",
    "        ],\n",
    "        verbose=verbose,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "\n",
    "We are given data on `dcoilwtico` and `onpromotion` for the test window. However, for a real deployment of our forecasting pipeline we would NOT know the future values for either the price of oil nor number of products on promotion nor transactions. If we want to use these series to create features for training our forecasting pipeline, then we need to also be able to generate forecasts of these variables prior to generating predictions from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.642784Z",
     "iopub.status.busy": "2023-10-12T04:58:14.641584Z",
     "iopub.status.idle": "2023-10-12T04:58:14.651003Z",
     "shell.execute_reply": "2023-10-12T04:58:14.649889Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.642711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def forecast_dcoilwtico():\n",
    "    test_df = load_test_df(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\n",
    "    oil_df = load_oil_df(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\")\n",
    "    forecast_df = (\n",
    "        test_df.join(oil_df, how=\"left\", on=[\"date\"])\n",
    "        .groupby([\"store_nbr\", \"family\"])\n",
    "        .fillna(method=\"ffill\")\n",
    "        .loc[:, [\"dcoilwtico\"]]\n",
    "    )\n",
    "    return forecast_df\n",
    "\n",
    "\n",
    "def forecast_onpromotion():\n",
    "    test_df = load_test_df(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\", onpromotion=True)\n",
    "    forecast_df = test_df.loc[:, [\"onpromotion\"]]\n",
    "    return forecast_df\n",
    "\n",
    "\n",
    "def forecast_transactions():\n",
    "    test_df = load_test_df(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\n",
    "    train_df = load_train_df(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\n",
    "    transactions_df = load_transactions_df(\"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\")\n",
    "\n",
    "    joined_df = train_df.join(transactions_df, how=\"left\", on=[\"store_nbr\", \"date\"])\n",
    "    concat_df = pd.concat(\n",
    "        [joined_df, test_df],\n",
    "        axis=0,\n",
    "        sort=True,\n",
    "    ).sort_index(axis=0)\n",
    "    forecast_df = (\n",
    "        concat_df.groupby([\"store_nbr\", \"family\"])\n",
    "        .fillna(method=\"ffill\")\n",
    "        .loc[pd.IndexSlice[:, :, \"2017-08-16\":], [\"transactions\"]]\n",
    "    )\n",
    "    return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.660598Z",
     "iopub.status.busy": "2023-10-12T04:58:14.660279Z",
     "iopub.status.idle": "2023-10-12T04:58:14.668784Z",
     "shell.execute_reply": "2023-10-12T04:58:14.667678Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.660574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _pipeline = make_lagged_feature_engineering_pipeline(\n",
    "#    max_lags=28,\n",
    "#    dcoilwtico_forecast_df=forecast_dcoilwtico(),\n",
    "#    onpromotion_forecast_df=forecast_onpromotion(),\n",
    "#    transactions_forecast_df=forecast_transactions(),\n",
    "#    n_jobs=-1,\n",
    "#    verbose=True\n",
    "# )\n",
    "# _df = _pipeline.fit_transform(_preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.681593Z",
     "iopub.status.busy": "2023-10-12T04:58:14.681219Z",
     "iopub.status.idle": "2023-10-12T04:58:14.688297Z",
     "shell.execute_reply": "2023-10-12T04:58:14.685896Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.681565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.71404Z",
     "iopub.status.busy": "2023-10-12T04:58:14.713143Z",
     "iopub.status.idle": "2023-10-12T04:58:14.721293Z",
     "shell.execute_reply": "2023-10-12T04:58:14.720019Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.714007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_feature_encoding_transformer(n_jobs=-1, verbose=True):\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"One-hot encode city feature\", OneHotEncoder(sparse_output=False, dtype=np.uint8), [\"city\"]),\n",
    "            (\"One-hot encode state feature\", OneHotEncoder(sparse_output=False, dtype=np.uint8), [\"state\"]),\n",
    "            (\n",
    "                \"One-hot encode store_cluster feature\",\n",
    "                OneHotEncoder(sparse_output=False, dtype=np.uint8),\n",
    "                [\"store_cluster\"],\n",
    "            ),\n",
    "            (\"One-hot encode store_type feature\", OneHotEncoder(sparse_output=False, dtype=np.uint8), [\"store_type\"]),\n",
    "        ],\n",
    "        n_jobs=n_jobs,\n",
    "        remainder=\"drop\",\n",
    "        verbose=verbose,\n",
    "        verbose_feature_names_out=False,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.769127Z",
     "iopub.status.busy": "2023-10-12T04:58:14.767924Z",
     "iopub.status.idle": "2023-10-12T04:58:14.776531Z",
     "shell.execute_reply": "2023-10-12T04:58:14.775283Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.769076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_feature_engineering_feature_union(\n",
    "    max_lags, dcoilwtico_forecast_df, onpromotion_forecast_df, transactions_forecast_df, n_jobs=-1, verbose=True\n",
    ") -> FeatureUnion:\n",
    "\n",
    "    feature_union = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            (\n",
    "                \"Passthrough sales target\",\n",
    "                ColumnTransformer(\n",
    "                    transformers=[(\"Identity transform\", FunctionTransformer(func=lambda df: df), [\"sales\"])],\n",
    "                    n_jobs=1,\n",
    "                    remainder=\"drop\",\n",
    "                    verbose=verbose,\n",
    "                    verbose_feature_names_out=False,\n",
    "                ),\n",
    "            ),\n",
    "            (\"Engineer date features\", make_date_features_transformer(verbose)),\n",
    "            (\"Engineer holidays and events features\", make_holidays_events_feature_union(n_jobs, verbose)),\n",
    "            (\n",
    "                \"Engineer lagged features\",\n",
    "                make_lagged_feature_engineering_pipeline(\n",
    "                    max_lags, dcoilwtico_forecast_df, onpromotion_forecast_df, transactions_forecast_df, n_jobs, verbose\n",
    "                ),\n",
    "            ),\n",
    "            (\"Encode categorical features\", make_feature_encoding_transformer(n_jobs=n_jobs, verbose=verbose)),\n",
    "        ],\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=verbose,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return feature_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.829522Z",
     "iopub.status.busy": "2023-10-12T04:58:14.828849Z",
     "iopub.status.idle": "2023-10-12T04:58:14.834333Z",
     "shell.execute_reply": "2023-10-12T04:58:14.833482Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.829478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _feature_union_kwargs = {\n",
    "#    \"max_lags\": 28,\n",
    "#    \"dcoilwtico_forecast_df\": forecast_dcoilwtico(),\n",
    "#    \"onpromotion_forecast_df\": forecast_onpromotion(),\n",
    "#    \"transactions_forecast_df\": forecast_transactions(),\n",
    "#    \"n_jobs\": -1,\n",
    "#    \"verbose\": True\n",
    "# }\n",
    "# _feature_union = make_feature_engineering_feature_union(\n",
    "#    **_feature_union_kwargs\n",
    "# )\n",
    "# _df = _feature_union.fit_transform(_preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.898035Z",
     "iopub.status.busy": "2023-10-12T04:58:14.896263Z",
     "iopub.status.idle": "2023-10-12T04:58:14.902668Z",
     "shell.execute_reply": "2023-10-12T04:58:14.901335Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.897995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for converting to Nixlats format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.942943Z",
     "iopub.status.busy": "2023-10-12T04:58:14.9424Z",
     "iopub.status.idle": "2023-10-12T04:58:14.954727Z",
     "shell.execute_reply": "2023-10-12T04:58:14.953466Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.942916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _create_unique_id(df):\n",
    "    unique_ids = (\n",
    "        df.loc[:, [\"store_nbr\", \"family\"]]\n",
    "        .apply(lambda feature: feature.cat.codes, axis=0)\n",
    "        .apply(lambda cs: \"_\".join(str(c) for c in cs), axis=1)\n",
    "        .astype(\"category\")\n",
    "    )\n",
    "    return unique_ids\n",
    "\n",
    "\n",
    "def make_nixtlats_preparation_pipeline(verbose=True) -> Pipeline:\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"Reset multi-index\", FunctionTransformer(func=lambda df: df.reset_index())),\n",
    "            (\n",
    "                \"Convert store_nbr and family to category dtype\",\n",
    "                FunctionTransformer(func=lambda df: df.astype({\"store_nbr\": \"category\", \"family\": \"category\"})),\n",
    "            ),\n",
    "            (\n",
    "                \"Create unique_id column using store_nbr and family\",\n",
    "                FunctionTransformer(func=lambda df: df.assign(unique_id=_create_unique_id(df))),\n",
    "            ),\n",
    "            (\n",
    "                \"Encode remaining categorical features\",\n",
    "                ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\n",
    "                            \"One-hot encode store_nbr and family\",\n",
    "                            OneHotEncoder(sparse_output=False, dtype=np.uint8),\n",
    "                            [\"store_nbr\", \"family\"],\n",
    "                        )\n",
    "                    ],\n",
    "                    remainder=\"passthrough\",\n",
    "                    verbose=verbose,\n",
    "                    verbose_feature_names_out=False,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"Rename columns for use with nixlats libraries\",\n",
    "                FunctionTransformer(func=lambda df: df.rename(columns={\"date\": \"ds\", \"sales\": \"y\"})),\n",
    "            ),\n",
    "            (\"Sort the columns\", FunctionTransformer(func=lambda df: df.sort_index(axis=1))),\n",
    "            (\n",
    "                \"Split into train and test data sets\",\n",
    "                FunctionTransformer(func=lambda df: (df.query(\"ds < 20170816\"), df.query(\"ds >= 20170816\"))),\n",
    "            ),\n",
    "        ],\n",
    "        verbose=verbose,\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:14.99729Z",
     "iopub.status.busy": "2023-10-12T04:58:14.996893Z",
     "iopub.status.idle": "2023-10-12T04:58:15.006986Z",
     "shell.execute_reply": "2023-10-12T04:58:15.005488Z",
     "shell.execute_reply.started": "2023-10-12T04:58:14.997262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_sales_forecasting_data(\n",
    "    onpromotion=True,\n",
    "    join_oil=True,\n",
    "    join_transactions=True,\n",
    "    start=\"20150701\",\n",
    "    q=0.99,\n",
    "    max_lags=28,\n",
    "    dcoilwtico_forecast_df=None,\n",
    "    onpromotion_forecast_df=None,\n",
    "    transactions_forecast_df=None,\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    "):\n",
    "\n",
    "    # load the training, test, and transactions data sets\n",
    "    _train_df = load_train_df(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\n",
    "\n",
    "    # preprocess the data\n",
    "    data_preprocessing_pipeline = make_data_preprocessing_pipeline(\n",
    "        onpromotion, join_oil, join_transactions, start, q, verbose\n",
    "    )\n",
    "    _preprocessed_df = data_preprocessing_pipeline.fit_transform(_train_df)\n",
    "\n",
    "    # forecast future values of exogenous variables\n",
    "    if dcoilwtico_forecast_df is None:\n",
    "        dcoilwtico_forecast_df = forecast_dcoilwtico()\n",
    "    if onpromotion_forecast_df is None:\n",
    "        onpromotion_forecast_df = forecast_onpromotion()\n",
    "    if transactions_forecast_df is None:\n",
    "        transactions_forecast_df = forecast_transactions()\n",
    "\n",
    "    # engineer features\n",
    "    feature_engineering_feature_union = make_feature_engineering_feature_union(\n",
    "        max_lags, dcoilwtico_forecast_df, onpromotion_forecast_df, transactions_forecast_df, n_jobs, verbose\n",
    "    )\n",
    "    _with_engineered_features_df = feature_engineering_feature_union.fit_transform(_preprocessed_df)\n",
    "\n",
    "    # prepate for nixlats\n",
    "    nixtlats_preparation_pipeline = make_nixtlats_preparation_pipeline(verbose)\n",
    "    train_df, test_df = nixtlats_preparation_pipeline.fit_transform(_with_engineered_features_df)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for extracting static feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.071155Z",
     "iopub.status.busy": "2023-10-12T04:58:15.070823Z",
     "iopub.status.idle": "2023-10-12T04:58:15.077553Z",
     "shell.execute_reply": "2023-10-12T04:58:15.076669Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.071128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_static_feature_names(df) -> list:\n",
    "    is_static_feature = (\n",
    "        df.columns.str.startswith(\"city_\")\n",
    "        + df.columns.str.startswith(\"state_\")\n",
    "        + df.columns.str.startswith(\"store_nbr_\")\n",
    "        + df.columns.str.startswith(\"store_type_\")\n",
    "        + df.columns.str.startswith(\"store_cluster\")\n",
    "        + df.columns.str.startswith(\"family_\")\n",
    "    )\n",
    "    static_feature_names = df.columns[is_static_feature].to_list()\n",
    "    return static_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage\n",
    "\n",
    "Uncomment and run the cells below to prepare the data using my default settings and explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.142465Z",
     "iopub.status.busy": "2023-10-12T04:58:15.142061Z",
     "iopub.status.idle": "2023-10-12T04:58:15.147512Z",
     "shell.execute_reply": "2023-10-12T04:58:15.146354Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.142436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_df, test_df = prepare_sales_forecasting_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.201529Z",
     "iopub.status.busy": "2023-10-12T04:58:15.200401Z",
     "iopub.status.idle": "2023-10-12T04:58:15.205463Z",
     "shell.execute_reply": "2023-10-12T04:58:15.204712Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.201484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.244339Z",
     "iopub.status.busy": "2023-10-12T04:58:15.243736Z",
     "iopub.status.idle": "2023-10-12T04:58:15.259881Z",
     "shell.execute_reply": "2023-10-12T04:58:15.258696Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.24431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Detrender(BaseTargetTransform):\n",
    "\n",
    "    def __init__(self, kwargs):\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def _create_deterministic_process(self, df):\n",
    "        deterministic_process = DeterministicProcess(df.index.get_level_values(self.time_col), **self.kwargs)\n",
    "        return deterministic_process\n",
    "\n",
    "    def _linear_regression_fit(self, df):\n",
    "        estimator = LinearRegression(fit_intercept=False).fit(\n",
    "            df.drop(self.target_col, axis=1), df.loc[:, self.target_col]\n",
    "        )\n",
    "        return estimator\n",
    "\n",
    "    def _linear_regression_predict(self, df):\n",
    "        estimator = self.estimators_.loc[df.name, \"LinearRegression\"]\n",
    "        predictions_df = pd.DataFrame({self.target_col: estimator.predict(df)}, index=df.index.get_level_values(-1))\n",
    "        return predictions_df\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "\n",
    "        in_sample_targets_df = df.loc[:, [self.target_col]]\n",
    "\n",
    "        # create the deterministic processes\n",
    "        self.deterministic_processes_ = (\n",
    "            in_sample_targets_df.groupby(self.id_col)\n",
    "            .apply(self._create_deterministic_process)\n",
    "            .rename(\"DeterministicProcess\")\n",
    "            .to_frame()\n",
    "        )\n",
    "\n",
    "        # generate in-sample features\n",
    "        self.in_sample_features_ = self.deterministic_processes_.groupby(self.id_col).apply(\n",
    "            lambda df: df.loc[df.name, \"DeterministicProcess\"].in_sample()\n",
    "        )\n",
    "\n",
    "        # fit the linear regression models\n",
    "        self.estimators_ = (\n",
    "            in_sample_targets_df.join(\n",
    "                self.in_sample_features_,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .groupby(self.id_col)\n",
    "            .apply(self._linear_regression_fit)\n",
    "            .rename(\"LinearRegression\")\n",
    "            .to_frame()\n",
    "        )\n",
    "\n",
    "        # predict the in-sample trend values\n",
    "        in_sample_trend_forecasts_df = self.in_sample_features_.groupby(self.id_col).apply(\n",
    "            self._linear_regression_predict\n",
    "        )\n",
    "\n",
    "        # detrend the in-sample targets\n",
    "        in_sample_residuals_df = in_sample_targets_df.sub(in_sample_trend_forecasts_df)\n",
    "        transformed_df = df.assign(**{self.target_col: in_sample_residuals_df})\n",
    "\n",
    "        return transformed_df\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        out_of_sample_residuals_df = df.loc[:, [self.target_col]]\n",
    "\n",
    "        # compute out-of-sample features\n",
    "        h = out_of_sample_residuals_df.index.get_level_values(self.time_col).unique().size\n",
    "        self.out_of_sample_features_ = self.deterministic_processes_.groupby(self.id_col).apply(\n",
    "            lambda df: df.loc[df.name, \"DeterministicProcess\"].out_of_sample(steps=h)\n",
    "        )\n",
    "\n",
    "        # predict the out-of-sample trend values\n",
    "        out_of_sample_trend_forecasts_df = self.out_of_sample_features_.groupby(self.id_col).apply(\n",
    "            self._linear_regression_predict\n",
    "        )\n",
    "\n",
    "        # re-introduce the trend to the residuals\n",
    "        out_of_sample_forecasts_df = out_of_sample_residuals_df.add(out_of_sample_trend_forecasts_df)\n",
    "        transformed_df = df.assign(**{self.target_col: out_of_sample_forecasts_df})\n",
    "\n",
    "        return transformed_df\n",
    "\n",
    "    def inverse_transform_fitted(self, df):\n",
    "        in_sample_residuals_df = df.loc[:, [self.target_col]]\n",
    "\n",
    "        # predict the in-sample trend values\n",
    "        in_sample_trend_forecasts_df = self.in_sample_features_.groupby(self.id_col).apply(\n",
    "            self._linear_regression_predict\n",
    "        )\n",
    "\n",
    "        # re-introduce the trend to the residuals\n",
    "        in_sample_forecasts_df = in_sample_residuals_df.add(in_sample_trend_forecasts_df)\n",
    "        transformed_df = df.assign(**{self.target_col: in_sample_forecasts_df})\n",
    "\n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.26946Z",
     "iopub.status.busy": "2023-10-12T04:58:15.268987Z",
     "iopub.status.idle": "2023-10-12T04:58:15.279101Z",
     "shell.execute_reply": "2023-10-12T04:58:15.277945Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.269432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _deterministic_process_kwargs = {\n",
    "#    \"constant\": True,\n",
    "#    \"order\": 1,\n",
    "#    \"seasonal\": True,\n",
    "#    \"period\": 7,\n",
    "#    \"additional_terms\": [\n",
    "#        CalendarFourier(freq=\"M\", order=4)\n",
    "#    ],\n",
    "# }\n",
    "#\n",
    "# _transformer = Detrender(\n",
    "#    kwargs=_deterministic_process_kwargs\n",
    "# )\n",
    "# _transformer.set_column_names(\n",
    "#    id_col=[\"store_nbr\", \"family\"],\n",
    "#    time_col=\"date\",\n",
    "#    target_col=\"sales\"\n",
    "# )\n",
    "\n",
    "# detrended_target_df = (\n",
    "#    _transformer.fit_transform(\n",
    "#        _preprocessed_df.loc[pd.IndexSlice[:, :, :\"2017-08-15\"]]\n",
    "#    )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.285043Z",
     "iopub.status.busy": "2023-10-12T04:58:15.284614Z",
     "iopub.status.idle": "2023-10-12T04:58:15.292869Z",
     "shell.execute_reply": "2023-10-12T04:58:15.29208Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.285014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# _transformer.inverse_transform_fitted(detrended_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.295642Z",
     "iopub.status.busy": "2023-10-12T04:58:15.294886Z",
     "iopub.status.idle": "2023-10-12T04:58:15.306641Z",
     "shell.execute_reply": "2023-10-12T04:58:15.305311Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.295606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# out_of_sample_residuals_df = (\n",
    "#    _preprocessed_df.assign(sales=detrended_target_df.loc[:, [\"sales\"]])\n",
    "#                    .groupby([\"store_nbr\", \"family\"])\n",
    "#                    .fillna(method=\"ffill\")\n",
    "#                    .loc[pd.IndexSlice[:, :, \"2017-08-16\":]]\n",
    "# )\n",
    "#\n",
    "# _transformer.inverse_transform(out_of_sample_residuals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T04:58:15.368682Z",
     "iopub.status.busy": "2023-10-12T04:58:15.368319Z",
     "iopub.status.idle": "2023-10-12T04:58:15.373284Z",
     "shell.execute_reply": "2023-10-12T04:58:15.372228Z",
     "shell.execute_reply.started": "2023-10-12T04:58:15.368655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_growth_rate_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"log transform\",\n",
    "            FunctionTransformer(\n",
    "                func=np.log,\n",
    "                inverse_func=np.exp,\n",
    "                check_inverse=False,\n",
    "            ),\n",
    "        ),\n",
    "        (\"difference transform\", FunctionTransformer(func=lambda df: df.diff(periods=1))),\n",
    "        (\"lag transform\", FunctionTransformer(func=lambda df: df.shift(periods=1))),\n",
    "    ],\n",
    "    verbose=True,\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"growth rate transform\",\n",
    "            ColumnTransformer(\n",
    "                transformers=[(\"growth rate transform\", _growth_rate_pipeline, [\"usd_index\"])],\n",
    "                remainder=\"passthrough\",\n",
    "                verbose=True,\n",
    "                verbose_feature_names_out=False,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"lag transforms\",\n",
    "            ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\n",
    "                        \"weekly rolling mean\",\n",
    "                        FunctionTransformer(\n",
    "                            func=lambda df: df.rolling(on=\"date\", window=7)\n",
    "                            .mean()\n",
    "                            .rename(columns={\"usd_index\": \"usd_index_rolling_mean_lag1_window_size7\"})\n",
    "                        ),\n",
    "                        [\"date\", \"usd_index\"],\n",
    "                    ),\n",
    "                    (\n",
    "                        \"two-week rolling mean\",\n",
    "                        FunctionTransformer(\n",
    "                            func=lambda df: df.rolling(on=\"date\", window=14)\n",
    "                            .mean()\n",
    "                            .rename(columns={\"usd_index\": \"usd_index_rolling_mean_lag1_window_size14\"})\n",
    "                        ),\n",
    "                        [\"date\", \"usd_index\"],\n",
    "                    ),\n",
    "                ],\n",
    "                remainder=\"passthrough\",\n",
    "                verbose=True,\n",
    "                verbose_feature_names_out=False,\n",
    "            ),\n",
    "        ),\n",
    "        (\"remove duplicate columns\", FunctionTransformer(func=lambda df: df.loc[:, ~df.columns.duplicated()])),\n",
    "    ],\n",
    "    verbose=True,\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "# X_df = _pipeline.fit_transform(usd_index_df)\n",
    "\n",
    "# train_df = (\n",
    "#    oil_df.merge(X_df, how=\"left\", on=[\"unique_id\", \"date\"])\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kret_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
